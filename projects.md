<ul>
  <li><a href="index">Home</a></li>
  <li><a class="active" href="projects">Projects</a></li>
  <li><a href="about">About</a></li>
  <li><a href="files/MuhammadUzairKhattak.pdf">Resume</a></li>
</ul>


<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in computer vision, machine learning, optimization, and image processing. Much of my research is about inferring the physical world (shape, motion, color, light, etc) from images. Representative papers are <span class="highlight">highlighted</span>.
              </p>
            </td>
          </tr>
        </tbody></table>
        
        
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            
        <tr>
            <td style="padding:20px;width:50%;vertical-align:middle">
                <div class="one">
                    <img src='images/EdgeNeXt.png' width="500">
                </div>
            </td>
            <td style="padding:20px;width:50%;vertical-align:middle">
                <a href="https://amshaker.github.io/EdgeNeXt/">
                    <font color="black"><strong>EdgeNeXt: Efficiently Amalgamated CNN-Transformer Architecture for Mobile Vision Applications</strong></font>
                </a>
                <br>
                <a href="https://scholar.google.com/citations?user=vTy9Te8AAAAJ&hl=en&authuser=1&oi=sra">
                                Muhammad Maaz*&Dagger;
                            </a>
                <strong><a href="https://amshaker.github.io/">Abdelrahman Shaker*&Dagger;</a></strong>,
                <a href="https://amshaker.github.io/">
                            </a>
                (*equal contribution)
                <a href="https://scholar.google.com/citations?hl=en&user=bZ3YBRcAAAAJ">
                                Hisham Cholakkal&dagger;
                            </a>
                 <a href="https://salman-h-khan.github.io/">
                                Salman Khan&dagger;
                            </a>
                <a href="https://www.waqaszamir.com/">
                                Syed Waqas Zamir&dagger;
                            </a>
                 <a href="https://scholar.google.com/citations?hl=en&authuser=1&user=_KlvMVoAAAAJ">
                                Rao Muhammad Anwer&dagger;
                            </a>
                <a href="https://scholar.google.es/citations?user=zvaeYnUAAAAJ&hl=en/">
                                Fahad Khan&dagger;
                            </a>
                
                <br>
                <a href="https://amshaker.github.io/EdgeNeXt/">Project page</a>
                /
                <a href="https://arxiv.org/abs/2206.10589">Paper</a>
                /
                <a href="https://github.com/mmaaz60/EdgeNeXt">Code & Model weights</a>
                <p></p>
                <p>We present EdgeNeXt, a new hybrid architecture that effectively combine the strengths of both CNN and Transformer models. Specifically in EdgeNeXt, we introduce split depth-wise transpose attention (SDTA) encoder that splits input tensors into multiple channel groups and utilizes depth-wise convolution along with self-attention across channel dimensions to implicitly increase the receptive field and encode multi-scale features. Our extensive experiments on classification, detection and segmentation tasks, reveal the merits of the proposed approach, outperforming state-of-the-art methods with comparatively lower compute requirements. Our EdgeNeXt model with 1.3M parameters achieves 71.2% top-1 accuracy on ImageNet-1K, outperforming MobileViT with an absolute gain of 2.2% with 28% reduction in FLOPs. Further, our EdgeNeXt model with 5.6M parameters achieves 81.1% (with knowledge distillation) and 79.4% (without knowledge distillation) top-1 accuracy on ImageNet-1K. </p>
            </td>
        </tr>    
        
                </tbody>
    </table>
